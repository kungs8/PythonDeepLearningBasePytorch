# -*- encoding: utf-8 -*-
'''
@File    :   5.3.3_批量正则化.py
@Time    :   2022/02/24 16:21:08
@Author  :   yanpenggong
@Version :   1.0
@Email   :   yanpenggong@163.com
@Copyright : 侵权必究
'''

# here put the import lib

# ----------------------------------------------------------------------------------------------------------------------
# 上文已经介绍了数据归一化，这个通常是针对输入数据而言。
# 但在实际训练过程中，经常出现隐含层因数据分布不均匀，导致梯度消失或不起作用的情况。
# 如采用 sigmoid函数 或 tanh函数 为激活函数时，如果数分布在两侧，这些机会函数的导数就接近于0。这样，BP算法得到的梯度也就消失了。
# 解决问题的方法：
#     Sergey Ioffe 和 Christian Szegedy 两位学者提出了批标准化(Batch Normalization) 方法。
#     Batch Normalization 不仅可以有效地解决梯度消失问题，而且还可以让调试超参数更加简单，在提高训练模型效率的同时，还可以让神经网络模型更加"健壮"。

# Batch Normalization(BN) 算法流程：
#     输入：微批次(mini-batch) 数据: B={x_{1}, x_{2}, ..., x_{m}}
#     学习参数：γ, β 类似于权重参数，可以通过梯度下降等算法求得。
#     其中 x_{i} 并不是网络的训练样本，而是指原网络中任意一个隐藏层激活函数的输入，这些输入是训练样本在网络中前向传播得来的。
#     输出：{y_{i}=NB_{γ, β}(x_{i})}
#     这里，求微批次样本均值: $μ <- \frac{1}{m}\sum_{i=1}^{m}x_{i}$
#     这里，求微批次样本方差: $δ_{B}^{2} <- \frac{1}{m}\sum_{i=1}^{m}(x_{i} - μ_{B})^{2}$
#     这里，对 x_{i}进行标准化处理：$\bar{x_{i}} <- \frac{x_{i} - μ_{B}}{\sqrt{δ_{B}^{2} + e}}$
#     这里，反标准化操作： $y_{i} = γ\bar{x_{i}} + β = NB_{γ, β}(x_{i})}$

# BN 是对隐藏层的标准化处理，它与输入的标准化处理 Normalizing Inputs 是有区别的。
#     - Normalizing Inputs是使所有输入的均值为0，方差为。
#     - Batch Normalization 可以使各隐藏层输入的均值和方差为任意值。
# 实际上，，从激活函数的角度来看，如果各隐藏层的输入均值在靠近0的区域，即处于激活函数的线性区域，这样不利于训练好的非线性神经网络，而且得到的模型效果也不会太好。
# 上面提到的 `对 x_{i}进行标准化处理`公式，就起到这个作用，当然它还有将归一化后的 x 还原的功能。

# - BN一般用在哪里？
#     用在非线性映射前，即对 $x = Wμ + b$做规范化时，在每一个全连接和激励函数之间。
# - 何时使用BN？
#     一般在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时，可以用 BN 来尝试解决。
# 一般情况下，也可以加入BN来加快训练速度，提高模型精度，还可以大大的提高训练模型的效率。

# BN 具体的功能：
#     1. 可以选择比较大的初始学习率，让训练速度飙升。
#         之前还需要慢慢地调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小点比例选择多少比较合适。
#         现在可以采用出事很大的学习率，然而学习率的衰减速度也很快，因为这个算法收敛很快。
#         当然，这个算法即使选择了较小的学习率，也比之前的收敛速度快，因为它具有快速训练收敛的特性。
#     2. 不用再去理会过拟合中 Dropout、L2正则项参数的选择问题。
#         采用 BN 算法后，可以移除这两项参数，或者可以选择更小的 L2正则约束参数了，因为 BN 具有提高网络泛化能力的特性。
#     3. 再也不需要使用局部响应归一化层。
#     4. 可以把训练数据彻底打乱。

# 添加 BN层 与 不添加 BN层 的网络结构代码
# 1. 不添加BN层：
#     net1_overfitting = torch.nn.Sequential(
#         torch.nn.Linear(13, 16),
#         torch.nn.ReLU(),
#         torch.nn.Linear(16, 32),
#         torch.nn.ReLU(),
#         torch.nn.Linear(32, 1)
#     )
# 2. 添加BN层
#     net1_bn = torch.nn.Sequential(
#         torch.nn.Linear(13, 16),
#         torch.nn.BatchNorm1d(num_features=16),
#         torch.nn.ReLU(),
#         torch.nn.Linear(16, 32),
#         torch.nn.ReLU(),
#         torch.nn.Linear(32, 1)
#     )

# 添加BN层对改善模型的泛化能力有一定的帮助，不过没有Dropout那么明显。
# 这个神经网络比较简单，BN在一些复杂网络中，效果会更好。