# -*- encoding: utf-8 -*-
'''
@File    :   5.4_选择合适激活函数.py
@Time    :   2022/02/24 17:57:54
@Author  :   yanpenggong
@Version :   1.0
@Email   :   yanpenggong@163.com
@Copyright : 侵权必究
'''

# here put the import lib

# ----------------------------------------------------------------------------------------------------------------------
# 激活函数在神经网络中作用有很多，主要作用是给神经网络提供非线性建模能力。
# 如果没有激活函数，那么再多层的神经网络也只能处理线性可分问题。
# 常用的激活函数有: sigmoid, tanh, relu, LeakyReLU, softmax等。

# 在搭建神经网络时，如何选择激活函数？如果搭建的神经网络层数不多，选择 sigmoid, tanh, relu, softmax 都可以；
# 而如果搭建的网络层次较多，那就需要小心，选择不当就可导致梯度消失问题。
# 此时一般不宜选择 sigmoid, tanh 激活函数，因为它们的导数都小于1，尤其是 sigmoid 的导数在 [0, 1/4] 之间，多层叠加后，
# 根据微积分链式法则，随着层数增多，导数或偏导将指数级变小。
# 所以层数较多的激活函数需要考虑其导数不宜小于 1 当然也不能大于 1，大于 1 将导致梯度爆炸，导数为 1 最好，而激活函数 relu 正好满足这个条件。
# 所以，搭建比较深的神经网络时，一般使用 relu 激活函数，当然一般神经网络也可使用。
# 此外，激活函数 softmax 由于 $\sum_{i}\delta_{i}^{z} = 1$， 常用于多分类神经网络输出层。

# 激活函数在 Pytorch 中使用示例：
# from torch import nn
# m = nn.Sigmoid()
# imput = torch.randn(2)
# output = m(input)
# 激活函数输入维度与输出维度是一样的。激活函数的输入维度一般包括批量数 N，即输入数据的维度一般是 4 维，如 (N, C, W, H)