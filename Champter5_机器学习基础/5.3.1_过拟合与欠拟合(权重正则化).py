# -*- encoding: utf-8 -*-
'''
@File    :   5.3.1_过拟合与欠拟合(权重正则化).py
@Time    :   2022/02/24 11:11:37
@Author  :   yanpenggong
@Version :   1.0
@Email   :   yanpenggong@163.com
@Copyright : 侵权必究
'''

# here put the import lib

# ----------------------------------------------------------------------------------------------------------------------
# 模型确定后，开始训练模型，然后对模型进行评估和优化，这个过程往往是循环往复的。
# 在训练模型过程中，经常会出现刚开始训练时，训练和测试精度不高(或损失值较大)，然后通过增加迭代次数或通过优化，训练精度和测试精度继续提升，如果是这样，最好。
# 但随着训练迭代次数的增加或不断优化，也可能会出现训练精度或损失值继续改善，但测试精度下降或损失值不降反升的情况。

# 出现这种情况，说明优化过头了，把训练数据中一些无关紧要甚至错误的模式也学到了。这就是通常说的出现过拟合。
# 机器学习中有很多解决方法，这些方法又统称为正则化。

# ----------------------------------------------------------------------------------------------------------------------
# 5.3.1 权重正则化
# 正则化是解决过拟合的其中一个有效方法。
# 正则化不仅可以有效的降低高方差，还有利于降低偏差。
# 机器学习中，很多被显示地用来减少测试误差的策略，统称为正则化。旨在减少泛化误差而不是训练误差。
# 传统意义上的正则化一般分为 L0, L1, L2, L∞ 等。
# Pytorch 实现的正则化(这里以实现L2为例)：
#     神经网络的 L2 正则化称为权重衰减(Weight Decay)。
#     torch.optim 集成了很多优化器，如: SGD、Adadelta、Adam、Adagrad、RMSprop 等。
#     torch.optim 中优化器自带的一个参数 weight_decay，用于知道权值衰减率，相当于 L2 正则化中的 λ 参数，也就是下式中的 λ：
#     $\inderset{\theta}{min} \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^{2} + λ||W||^{2}$
