# -*- encoding: utf-8 -*-
'''
@File    :   5.6_选择合适优化器.py
@Time    :   2022/04/12 10:52:53
@Author  :   yanpenggong
@Version :   1.0
@Email   :   yanpenggong@163.com
@Copyright : 侵权必究
'''

# here put the import lib
# ----------------------------------------------------------------------------------------------------------------------
# 优化器在机器学习、深度学习中往往起着举足轻重 的作用，同一个模型，因选择不同的优化器，性能有可能相差很大，甚至导致一些模型无法训练。
# 所以，了解各种优化器的基本原理非常必要。
# 本节将重点介绍各种优化器或算法的主要原理，及各自的优点或不足。

# ----------------------------------------------------------------------------------------------------------------------
# 5.6.1 传统梯度优化的不足
# 传统梯度更新算法为最常见、最简单的一种参数更新策略。
# 其基本思想是：先设定一个学校率 λ，参数延梯度的反方向移动。假设需更新的参数为 θ，梯度为g，则其更新策略可表示为：θ <- θ-λg
# 这种梯度更新算法简洁，当学习率取值恰当时，可以收敛到全面最优点(凸函数)或局部最优点(非凸函数)。

# 但其不足也很明显，对超参数学习率比较敏感(过小导致收敛速度过慢，过大又越过极值点)，要选中一个恰当的学习率往往要花费不少时间。

# 学习率除了敏感，有时还会因其在迭代过程中保持不变，很容易造成算法被卡在鞍点的位置。
# 另外，在较平坦的区域，优于梯度接近于0，优化算法会因误判，在还未到达极值点时，就提前结束迭代。

# 传统梯度优化方面的这些不足，在深度学习中会更加明显。为此，研究人员自然想到如何克服这些不足的问题。
# 影响优化的无非两个因素：
#     - 梯度方向
#     - 学习率
# 所以很多优化方法大多从这两方面入手，有些从梯度入手，如动量更新策略；而有些从学习率入手，这涉及调参问题；还有从两方面同时入手，如自适应更新策略，接下来分别结束这些方法。

# ----------------------------------------------------------------------------------------------------------------------
# 5.6.2 动量算法
# 梯度下降法在遇到平坦或高曲率区域时，学习过程有时很慢。利用动量算法能比较好解决这个问题。
# 不使用动量算法的SGD学习速度比较慢，振幅较大；而使用动量算法的SGD，振幅较小，而且会较快到达极值点。

# 动量(Momentum) 是模拟物理里动量的概念，具有物理上惯性的含义，一个物体在运动时具有惯性，把这个思想运用到梯度下降计算中，可以增加算法的收敛速度和稳定性。
# 动量算法每下降一步都是由前面下降方向的一个累积和当前点的梯度方向组合而成。含动量的随机梯度下降法。

# 既然每一步都要将两个梯度方向(历史梯度、当前梯度)做一个合并再下降，那为什么不先按照历史梯度往前走那么一小步，按照前面一小步位置的"超前梯度" 来做梯度合并呢？
# 这样就恶意先往前走一步，在靠前一点的位置看到梯度，然后按照那个位置再来修正这一步的梯度方向。
# 这就得到动量算法的一种改进算法，称为 Nesterov Accelerated Gradient,简称 NAG 算法。
# 这种预更新方法能防止大幅震荡，不会错过最小值，并会对参数更新更加敏感。
# NAG 动量法和经典动量法的差别就在B点和C点梯度不同。动量法更多关注梯度下降方法的优化，如果能从方向和学习率同时优化，效果或许更理想。
# 事实也确实如此，而且这些优化在深度学习中显得尤为重要。
# 接下来将介绍几种自适应优化算法，这些算法同时从梯度方向及学习率进行优化，效果都非常好。

# ----------------------------------------------------------------------------------------------------------------------
# 5.6.3 AdaGrad 算法
# 传统梯度下降算法对学习率这个超参数非常敏感，难以驾驭，对参数空间的某些方向也没有很好的方法。
# 这些不足在深度学习中，因高维空间、多层神经网络等因素，常会出现平坦、鞍点、悬崖等问题，因此，传统梯度下降法在深度学习中显得力不从心。
# 动量算法在一定程度上缓解了对参数某些方向的问题，但需要新增一个参数，而且对学习率对控制还不是很理想。为了更好的驾驭这个超参数，人们想出来多种自适应相应的情况。

# AdaGrad 算法是通过参数来调整合适的学习率λ，是能独立地自动调整模型参数的学习率，对稀疏参数进行大幅更新和对频繁参数进行小幅更新。
# 因此，AdaGrad 方法非常适合处理稀疏数据。AdaGrad算法在某些深度学习模型上效果不错。但还有些不足，可能因其累积梯度平方导致学习率过早或过量的减少所致。

# 注意：
#     - 随着迭代时间越长，累积梯度r越大，导致学习率随着时间减小，在接近目标值时，不会因为学习速率过大而越过极值点。
#     - 不同参数之间的学习速率不同，因此，与前面固定学习速率相比，不容易在鞍点卡住。
#     - 如果梯度累积参数r比较小，则学习速率会比较大，所以参数迭代的步长就会比较大。相反，如果梯度累积参数比较大，则学习率会比较小，所以迭代的步长会比较小。

# ----------------------------------------------------------------------------------------------------------------------
# 5.6.4 RMSprop算法
# RMProp算法通过修改AdaGrad 得来，其目的是在非凸背景下效果更好。针对梯度平方和累积越来越大的问题，RMProp指数加权的移动平均代替梯度平方和。
# RMProp 为了使用移动平均，还引入了一个新的超参数ρ，用来控制移动平均的长度范围。
# RMProp算法在实践中已被证明是一种有效且实用的深度神经网络优化算法，因而在深度学习中得到广泛的应用。

# ----------------------------------------------------------------------------------------------------------------------
# 5.6.5 Adam算法
# Adam(Adaptive Moment Estimation) 本质上是带有动量项的RMSProp，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。
# Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。

# Adam 是另一种学习速率自适应的深度神经网络方法，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习速率。

# 优化算法也是深度学习的核心之一。优化算法有很多，如随机梯度下降法、自适应优化算法等，那么具体该如何选择？
# RMSProp、Adadelta 和 Adam 被认为是自适应优化算法，因为它们会自动更新学习率。而使用SGD时，必须手动选择学习率和动量参数，通常会随着时间的推移而降低学习率。

# 有时可以考虑综合实验这些优化算法，如采用先使用Adam，然后使用SGD大优化方法，这个想法，实际上是由于在训练的早期阶段SGD对参数调整和初始化非常敏感。
# 因此，我们可以通过先使用Adam优化算法来进行训练，这将大大地节省训练时间，且不必担心初始化和参数调整，一旦用Adam训练获得较好的参数后，就可以切换到SGD+动量优化，以达到最佳性能。
# 采用这种方法有时能达到很好的效果。