# -*- encoding: utf-8 -*-
'''
@File    :   5.3.4_权重初始化.py
@Time    :   2022/02/24 17:33:28
@Author  :   yanpenggong
@Version :   1.0
@Email   :   yanpenggong@163.com
@Copyright : 侵权必究
'''

# here put the import lib

# ----------------------------------------------------------------------------------------------------------------------
# 传统机器学习算法中很多并不是采用迭代优化，因此需要初始化的内容不多。
# 但深度学习的算法一般采用迭代方法，而且参数多、层数多，所以很多算法不同程度上会受到初始化的影响。

# 初始化对训练的影响：
#     初始化能决定算法是否收敛：
#         - 如果算法的初始化不适当，初始值过大肯能会在前向传播 或 反向传播中产生爆炸的值；
#         - 如果太小将导致丢失信息。
#     对收敛的算法适当的初始化能加快收敛速度。
#     初始值的选择将影响模型收敛局部最小值还是全局最小值。
#     初始值不同，导致收敛到不同的极值点。初始化也可以影响模型的泛化。

# 对权重、偏移量进行初始化：(初始化这些参数是否有一般性原则?)
#     常见的参数初始化有零值初始化、随机初始化、均值分布初始化、正态分布初始 和 正交分布初始等。
#     一般采用 正态分布 或 均匀分布 的初始值，实践表明正态分布、正交分布、均匀分布 的初始值能带来更好的效果。

# 继承 nn.Module 的模块参数都采取了较合理的初始化策略，一般情况使用其缺省初始化策略就足够了。
# 当然，如果想要修改，Pytorch 也提供了 nn.init 模块，该模块提供了常用的初始化策略，
# (eg: xavier、kaiming 等经典初始化策略)，使用这些初始化策略有利于激活值的分布呈现出更有广度或更贴近正态分布。
#     - xavier: 一般用于激活函数是S型(eg: sigmoid、tanh)的权重初始化。
#     - kaiming: 更适合于激活函数为ReLU类的权重初始化。