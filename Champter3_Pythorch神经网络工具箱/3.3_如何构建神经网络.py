# -*- encoding: utf-8 -*-
'''
@File    :   3.3_如何构建神经网络.py
@Time    :   2020/06/17 13:43:03
@Author  :   yanpenggong
@Version :   1.0
@Contact :   yanpenggong@163.com
'''

# here put the import lib
import torch

# -------------------------------------------------------------------------------------------------------------------------------------------------------
# 3.3.1 构建神经网络层
# 3.2实例中，采用了torch.nn.Sequential()来构建网络层，这个有点类似Keras的models.Sequential()，使用起来像搭积木一样，非常方便。
# 不过这种方法每层的编码是默认的数字，不易区分。
# 如果要对每层定义一个名称，可以采用Sequential的一种改进方法，在Sequential的基础上，通过add_module()添加每一层，并为每一层添加一个单独的名字。
# 还可以在Sequential基础上，通过字典的形式添加每一层，并且设置单独的层名称。

# 以下采用字典方式构建网络的一个示例
class Net(torch.nn.Module):
    def __init__(self):
        super(Net4, self).__init__()
        self.conv = torch.nn.Sequential(
            OrderedDict(
                [
                    ("conv1", torch.nn.Conv2d(3, 32, 3, 1, 1)),
                    ("relu1", torch.nn.ReLU()),
                    ("pool", torch.nn.MaxPool2d(2))
                ]
            )
        )
        self.dense = torch.nn.Sequential(
            OrderedDict(
                [
                    ("dense1", torch.nn.Linear(32*3*3, 128)),
                    ("relu2", torch.nn.ReLU()),
                    ("dense2", torch.nn.Linear(128, 10))
                ]
            )
        )

# -------------------------------------------------------------------------------------------------------------------------------------------------------
# 3.3.2 反向传播
# 定义好每层后，还需要通过前向传播的方式把这些串起来。这就涉及到如何定义forward函数的问题。
# forward函数的任务是把输入层、网络层、输出层链接起来，实现信息的前向传导。该函数的参数一般为输入参数，返回值为输出数据。
# forward函数中，有些层来自nn.Module，也可以使用nn.functional定义。来自nn.Module需要实例化，使用nn.functional定义的可以直接使用。

# -------------------------------------------------------------------------------------------------------------------------------------------------------
# 3.2.3 反向传播
# 前向传播函数定义好之后，接下来就是梯度的反向传播。
# 这里关键是利用复合函数的链式法则。深度学习中涉及到很多函数，如果要自己手工实现反向传播，比较耗时。
# PyTorch提供了自动反向传播的功能，使用nn工具箱，无需我们自己编写反向传播，直接让损失函数(loss)调用backward()即可，非常方便、高效
# 在反向传播中，优化器是一个重要角色。优化方法有很多种除了SGD，还有其它的优化器

# -------------------------------------------------------------------------------------------------------------------------------------------------------
# 3.3.4 训练模型
# 层、模型、损失函数和优化器等都定义或创建好后，接下来就是训练模型。

# 训练模型时需要注意使模型处于训练模式，即调用model.train()。  调用model.train()会把所有的module设置为训练模式。
# 如果是测试或验证阶段，需要使模型处于验证阶段，即调用model.eval()。  调用model.eval()会把所有的training属性设置为False

# 缺省情况下梯度是累加的，需要手工把梯度初始化或清零，调用optimizer.zero_grad()即可。
# 训练过程中，正向传播生成网络的输出，计算输出和实际值之间的损失值。调用loss.backward()自动生成梯度，然后使用optimizer.step()执行优化器，把梯度传播回每个网络
# 使用GPU训练，需要把模型、训练数据、测试数据发送到GPU上，即调用.to(device)
# 如果需要使用多GPU进行处理，可使模型或相关数据引用nn.DataParallel(具体在第4章介绍)